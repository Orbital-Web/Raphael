# Raphael NNUE

This document contains all the information for the various Raphael NNUE iterations.

## Raphael 2.0

### Basilisk v1

**Architecture:** `(768 -> 64) x2 -> 1`

**Quantization:** `QA=255, QB=64`

**Training Parameters:**

```text
lr:  linear-decay from 0.001 to 0.001 * 0.3^5
wdl: 0.3
```

**Notes:**
> This is the very first Raphael NNUE, and it was trained using a custom datagen script and a custom trainer written using Pytorch.
>
> The data was generated by evaluating around 200 million positions from public CCRL matches at 5000 softnodes using Raphael v1.8 (HCE). Then, positions in check and positions where the bestmove is a capture were filtered out, leaving around 150 million positions.

## Raphael 3.1

### Basilisk v2

**Training Parameters:**

```text
lr:  cosine-decay from 0.001 to 0.001 * 0.3^5
wdl: 0.4
```

**Results:**

```text
Elo   | 155.84 +- 23.70 (95%)
SPRT  | 8.0+0.08s Threads=1 Hash=16MB
LLR   | 2.89 (-2.25, 2.89) [0.00, 5.00]
Games | N: 454 W: 250 L: 59 D: 145
Penta | [2, 9, 66, 96, 54]
```

**Notes:**
> Same arch as v1, but data comes from self-played games at 5000 softnodes. Postions were filtered using the default viriformat filter in bullet, but with `min_pieces=2`.
>
> Around 6m games were played with around 600m positions pre-filtering. The net was trained using bullet with basically the same settings as 1-simple.rs, apart from a few training and model parameters.

### Basilisk v3

**Training Parameters:**

```text
lr:  cosine-decay from 0.001 to 0.001 * 0.3^5 with warmup for 1600 batches
wdl: 0.4
```

**Results:**

```text
Elo   | 16.51 +- 8.00 (95%)
SPRT  | 8.0+0.08s Threads=1 Hash=16MB
LLR   | 2.91 (-2.25, 2.89) [0.00, 5.00]
Games | N: 3074 W: 943 L: 797 D: 1334
Penta | [51, 343, 637, 421, 85]
```

**Notes:**
> Same data as v2 but with horizontal mirroring. Some minor changes in the SIMD code (along with the necessary inference changes for mirroring) to make better use of registers which the compiler probably does already.
